{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"../README.md CRAWLING Pengertian web crawler- atau sering juga disebut spiders adalah sebuah tool untuk mengindeks dan mengunduh konten dari internet, lalu disimpan kedalam database mesin pencarian. Beberapa web crawler lain selain Googlebot adalah sebagi berikut : 1. Bingbot dari Bing 2. Slurp Bot dari Yahoo 3. DuckDuckBot dari DuckDuckGO 4. Baiduspider dari Baidu (mesin pencari dari cina) 5. Yandex Bot dari Yandex (mesin pencari dari rusia) 6. Sogou Spider dari Sogou (mesin pencari dari cina) 7. Exabot dari Exalead 8. Alexa Crawler dari Amazona Bagaimana Cara Kerja Crawler? Internet selalu berubah dan berkembang setiap waktunya. Karena tak memungkinkan untuk mengetahui jumlah pasti beberapa banyak halaman yang ada di internet, web crawler ini memulai pekerjaannya berdasarkan daftar link halaman yang sudah iya kenal sebelumnya dari sitemap suatu website. Dari daftar link sitemap tersebut, ia akan menemukan link-link lain yaang tersebar di dalamnya. Setelah itu, ia akan melakukan crawling ke link-link yang baru saja ditemukan itu. Proses ini akan terulang lagi di link selanjutnya dan bisa terus berjalan tanpa henti. Namun web crawler ini tidak sembarangan melakukan crawling. Ada beberapa aturan yang tetap harus mereka patuhi, sehingga merka bisa lebih selektif dalam crawling. Biasanya dalam melakukan crawling, ia mempertimbangkan tiga hal yaitu : Seberapa penting dan relavan suatu halaman Kunjungan rutin Menuruti keinginan Robots.txt Robots.txt ini merupakan file di sebuah website yang berisi informasi mengenai halaman mana yang boleh diindeks dan halaman mana yang tidak boleh. Fungsi Web Crawler Fungsi utama dari web crawler memang mengindeks konten di internet. Namun disamping itu, ada beberapa fungsi lain yang juga tidak kalah penting : Membandingkan Harga Data untuk Tools Analisis Data untuk Statistik Crawling Data Twitter Menggunakan Python untuk crawling data twitter kita akan menggunakan library Tweepy . untuk menginstall Tweepy ada tiga cara : Via pip \u200b pip install tweepy Via clone repository dari github \u200b git clone https://github.com/tweepy/tweepy.git \u200b cd tweepy \u200b pip install install langsung dari repository github \u200b pip install git+https://github.com/tweepy/tweepy.git Saya merekomendasikan menggunakan pip, karena lebih simple. selanjutnya kita bisa melihat source code dibawah ini. import tweepy import csvaccess_token=\"\" access_token_secret=\"\" consumer_key=\"\" consumer_key_secret=\"\"auth = tweepy.OAuthHandler(consumer_key,consumer_key_secret) api = tweepy.API(auth)# Open/create a file to append data to csvFile = open('nama-file.csv', 'w', encoding='utf-8')#Use csv writer csvWriter = csv.writer(csvFile)for tweet in tweepy.Cursor(api.search, q='#Python -filter:retweets', tweet_mode='extended',lang=\"id\", since='2021-01-01', until='2021-01-10').items(100): text = tweet.full_text user = tweet.user.name created = tweet.created_at csvWriter.writerow([created, text.encode('utf-8'), user]) csvWriter = csv.writer(csvFile) csvFile.close() Kesimpulan untuk sebuah tool yang bekerja dibalik layar tanpa henti, web crawler ini memberikan banyak manfaat, bukan ? setelah mengetahui banyak manfaatnya, anda pasti menginginkan web crawler mengindeks ke website anda. Unrtuk membuat web crawler mengindeks website anda, maka anda perlu mengoptimasi website anda. Baik dari aspek SEO, desain, hingga responsivitas website anda. Referensi https://www.niagahoster.co.id/blog/apa-itu-web-crawler/ https://blog.javan.co.id/crawling-data-twitter-menggunakan-python-postman-cffbf14f962c","title":"Home"},{"location":"#crawling","text":"Pengertian web crawler- atau sering juga disebut spiders adalah sebuah tool untuk mengindeks dan mengunduh konten dari internet, lalu disimpan kedalam database mesin pencarian. Beberapa web crawler lain selain Googlebot adalah sebagi berikut : 1. Bingbot dari Bing 2. Slurp Bot dari Yahoo 3. DuckDuckBot dari DuckDuckGO 4. Baiduspider dari Baidu (mesin pencari dari cina) 5. Yandex Bot dari Yandex (mesin pencari dari rusia) 6. Sogou Spider dari Sogou (mesin pencari dari cina) 7. Exabot dari Exalead 8. Alexa Crawler dari Amazona Bagaimana Cara Kerja Crawler? Internet selalu berubah dan berkembang setiap waktunya. Karena tak memungkinkan untuk mengetahui jumlah pasti beberapa banyak halaman yang ada di internet, web crawler ini memulai pekerjaannya berdasarkan daftar link halaman yang sudah iya kenal sebelumnya dari sitemap suatu website. Dari daftar link sitemap tersebut, ia akan menemukan link-link lain yaang tersebar di dalamnya. Setelah itu, ia akan melakukan crawling ke link-link yang baru saja ditemukan itu. Proses ini akan terulang lagi di link selanjutnya dan bisa terus berjalan tanpa henti. Namun web crawler ini tidak sembarangan melakukan crawling. Ada beberapa aturan yang tetap harus mereka patuhi, sehingga merka bisa lebih selektif dalam crawling. Biasanya dalam melakukan crawling, ia mempertimbangkan tiga hal yaitu : Seberapa penting dan relavan suatu halaman Kunjungan rutin Menuruti keinginan Robots.txt Robots.txt ini merupakan file di sebuah website yang berisi informasi mengenai halaman mana yang boleh diindeks dan halaman mana yang tidak boleh. Fungsi Web Crawler Fungsi utama dari web crawler memang mengindeks konten di internet. Namun disamping itu, ada beberapa fungsi lain yang juga tidak kalah penting : Membandingkan Harga Data untuk Tools Analisis Data untuk Statistik Crawling Data Twitter Menggunakan Python untuk crawling data twitter kita akan menggunakan library Tweepy . untuk menginstall Tweepy ada tiga cara : Via pip \u200b pip install tweepy Via clone repository dari github \u200b git clone https://github.com/tweepy/tweepy.git \u200b cd tweepy \u200b pip install install langsung dari repository github \u200b pip install git+https://github.com/tweepy/tweepy.git Saya merekomendasikan menggunakan pip, karena lebih simple. selanjutnya kita bisa melihat source code dibawah ini. import tweepy import csvaccess_token=\"\" access_token_secret=\"\" consumer_key=\"\" consumer_key_secret=\"\"auth = tweepy.OAuthHandler(consumer_key,consumer_key_secret) api = tweepy.API(auth)# Open/create a file to append data to csvFile = open('nama-file.csv', 'w', encoding='utf-8')#Use csv writer csvWriter = csv.writer(csvFile)for tweet in tweepy.Cursor(api.search, q='#Python -filter:retweets', tweet_mode='extended',lang=\"id\", since='2021-01-01', until='2021-01-10').items(100): text = tweet.full_text user = tweet.user.name created = tweet.created_at csvWriter.writerow([created, text.encode('utf-8'), user]) csvWriter = csv.writer(csvFile) csvFile.close() Kesimpulan untuk sebuah tool yang bekerja dibalik layar tanpa henti, web crawler ini memberikan banyak manfaat, bukan ? setelah mengetahui banyak manfaatnya, anda pasti menginginkan web crawler mengindeks ke website anda. Unrtuk membuat web crawler mengindeks website anda, maka anda perlu mengoptimasi website anda. Baik dari aspek SEO, desain, hingga responsivitas website anda. Referensi https://www.niagahoster.co.id/blog/apa-itu-web-crawler/ https://blog.javan.co.id/crawling-data-twitter-menggunakan-python-postman-cffbf14f962c","title":"CRAWLING"}]}